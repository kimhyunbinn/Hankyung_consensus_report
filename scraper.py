import os
import requests
import telegram
import asyncio
import re
import fitz
import json
from bs4 import BeautifulSoup
from io import BytesIO

# --- ì„¤ì • ---
TELEGRAM_TOKEN = os.environ.get('TELEGRAM_TOKEN')
CHAT_ID = os.environ.get('CHAT_ID')
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
BASE_URL = "https://consensus.hankyung.com"

def get_summary_rest(text):
    if not GEMINI_API_KEY: return "âŒ í‚¤ ë¯¸ì„¤ì •"
    # ëª¨ë¸ ì¸ì‹ë¥ ì´ ê°€ì¥ ë†’ì€ v1beta í‘œì¤€ ê²½ë¡œ
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    
    payload = {
        "contents": [{"parts": [{"text": f"ë„ˆëŠ” ê¸ˆìœµ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ë‚´ìš©ì„ 3ê°€ì§€ í•µì‹¬ í¬ì¸íŠ¸ë¡œ ìš”ì•½í•´ì¤˜:\n\n{text[:8000]}"}]}]
    }
    
    try:
        # í‚¤ ë’·ìë¦¬ë¥¼ ë¡œê·¸ì— ì°ì–´ ì‹¤ì œ ì ìš©ëœ í‚¤ í™•ì¸ (ë³´ì•ˆìƒ ë’¤ 4ìë¦¬ë§Œ)
        key_hint = GEMINI_API_KEY[-4:] if GEMINI_API_KEY else "None"
        res = requests.post(url, json=payload, timeout=20)
        
        if res.status_code == 200:
            return res.json()['candidates'][0]['content']['parts'][0]['text'].strip()
        else:
            return f"âŒ ìš”ì•½ ì‹¤íŒ¨ (Code {res.status_code})\nì‚¬ìš©ì¤‘ì¸ í‚¤ ë’·ìë¦¬: {key_hint}\nì—ëŸ¬ë‚´ìš©: {res.text[:100]}"
    except Exception as e:
        return f"âŒ í†µì‹  ì˜¤ë¥˜: {str(e)}"

def get_pdf_text(pdf_url):
    try:
        res = requests.get(pdf_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
        with fitz.open(stream=BytesIO(res.content), filetype="pdf") as doc:
            return "".join([p.get_text() for p in doc[:3]])
    except: return ""

async def main():
    bot = telegram.Bot(token=TELEGRAM_TOKEN)
    targets = [{"n":"ì‚°ì—…", "i":"ğŸ—ï¸", "t":"industry"}, {"n":"ì‹œì¥", "i":"ğŸ“ˆ", "t":"market"}]
    
    print("ğŸš€ ìˆ˜ë™ ìš”ì•½ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    for cat in targets:
        url = f"https://consensus.hankyung.com/analysis/list?skinType={cat['t']}"
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select('tr')[1:3] # ìµœì‹  2ê°œì”©
        
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 5: continue
            
            a = row.find('a', href=re.compile(r'report_idx='))
            title = a.get_text(strip=True)
            
            # ì¶œì²˜(ì¦ê¶Œì‚¬) ì°¾ê¸° ê°•í™”
            provider = "ì¶œì²˜ë¯¸ìƒ"
            for i in [4, 5, 3]:
                val = cols[i].get_text(strip=True)
                if val and not any(x.isdigit() for x in val.replace('.','')):
                    provider = val
                    break
            
            full_link = BASE_URL + a['href'] if a['href'].startswith('/') else a['href']
            summary = get_summary_rest(get_pdf_text(full_link))
            
            msg = (f"<b>{cat['i']} {cat['n']} ë¦¬í¬íŠ¸</b>\n\n"
                   f"ì¶œì²˜: <b>{provider}</b>\n"
                   f"ì œëª©: {title}\n"
                   f"--------------------------\n"
                   f"{summary}\n"
                   f"--------------------------\n"
                   f"<a href='{full_link}'>ğŸ‘‰ ì›ë¬¸ ë³´ê¸°</a>")
            
            await bot.send_message(chat_id=CHAT_ID, text=msg, parse_mode='HTML', disable_web_page_preview=True)
            await asyncio.sleep(1)

if __name__ == "__main__":
    asyncio.run(main())
